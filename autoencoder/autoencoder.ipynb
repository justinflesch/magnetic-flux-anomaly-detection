{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Deep learning is a technique which takes complex algebraic circuits with tunable connection strength. \"Deep\" refers to the many circuits of **layers** which requires many input and output computations. It is the most widely used appoach: **feedforward neural network**, **recurrent neural networks**, **linear neural network**, **long-short term memory neural network**, and **convolution neural network**. With these approaches, we can apply **autoencoding**.\n",
    "\n",
    "## Basic Neural Network\n",
    "\n",
    "We can demonstrate the basic of neural networks by applying a simple implementation using only the `numpy` library. We want to train with deep learing learning methods known as a **neural network**.\n",
    "\n",
    "We will demonstrate this in a simple feedforward neural network. It has one direction that are acyclic with designated input and output nodes. Each node uses a fucntion and passes the results to the output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork:\n",
    "\n",
    "  def __init__(self, input_dim, output_dim, layer_table):\n",
    "\n",
    "    self.layers = [LinearLayer(input_dim, layer_table[0][1], layer_table[0][2])]\n",
    "    # print(\"First layer i/o:\",layer_table[0][1], layer_table[2][1])\n",
    "    size = len(layer_table)\n",
    "    # odd values of \"i\" must be linearLayers\n",
    "    for i in range(0, size-1):\n",
    "      # each activation function needs a subsequent LinearLayer object\n",
    "      self.layers.append(layer_table[i][0]())\n",
    "      self.layers.append(LinearLayer(layer_table[i][1], layer_table[i+1][1], layer_table[i][2]))\n",
    "\n",
    "      # The last LinearLayer object needs to be the output dim\n",
    "    self.layers.append(layer_table[size-1][0]())\n",
    "    self.layers.append(LinearLayer(layer_table[size-1][1], output_dim, layer_table[size-1][2]))\n",
    "\n",
    "      \n",
    "    \n",
    "    #append the last layer\n",
    "    # check if it works\n",
    "    for i in range(len(self.layers)):\n",
    "      print(type(self.layers[i]))\n",
    "    \n",
    "  def forward(self, X):\n",
    "    for layer in self.layers:\n",
    "      X = layer.forward(X)\n",
    "    return X\n",
    "\n",
    "  def backward(self, grad):\n",
    "    for layer in reversed(self.layers):\n",
    "      grad = layer.backward(grad)\n",
    "\n",
    "  def step(self):\n",
    "    for layer in self.layers:\n",
    "      layer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first layer is a linear layer, followed by a listing of the hidden layers. We can create a listing of the **activation functions** and the **dimensions** of the hidden layers of the neural network. We can also add the **learning rate of these layers** (typically the learning rate is the same throughout the neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  hp_tuple = [(ReLU, 128, 0.01), (ReLU, 128, 0.01), (Sigmoid, 128, 0.01)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node in a network is called a **unit**. The unit calculates the weighted sum of the inputs from the predecessors nodes and then applies a nonlinear function to product an output. let $a_j$ denote the output of the unit $j$ and let $w_{i,j}$ be the weight attached to the link from unit $i$ to unit $j$; then we have\n",
    "$$a_j = g_{j}(\\sum_{i}w_{i,j}a_i) \\equiv g_{j}(in_{j})$$\n",
    "\n",
    "The activation functions are as follows\n",
    "\n",
    "- sigmoid:\n",
    "$$\\sigma(x) = 1 / (1 + e^x)$$\n",
    "- ReLU:\n",
    "$$ReLU(x) = max(0,x)$$\n",
    "- softplus:\n",
    "$$ softplus(x) = log(1 + e^x)$$\n",
    "\n",
    "We can implement this in code as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "  def forward(self, input):\n",
    "    self.act = 1/(1+np.exp(-input))\n",
    "    return self.act\n",
    "  def backward(self, grad):\n",
    "    return grad * self.act * (1-self.act)\n",
    "  def step(self):\n",
    "    return\n",
    "\n",
    "class ReLU:\n",
    "  def forward(self, input):\n",
    "    self.mask = (input > 0)\n",
    "    return input * self.mask\n",
    "  def backward(self, grad):\n",
    "    return grad * self.mask\n",
    "  def step(self):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoding\n",
    "\n",
    "Autoencoding has two parts: an encoder to a representation and a decoder that maps from representation to observed data x.\n",
    "\n",
    "![Image](../images/autoencoder.jpg)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
